{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPRug3HOw-oX"
   },
   "source": [
    "## LSTM Language Model for Kim Possible Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJASEzg7w-oY"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import chainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import re\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyperparameters\n",
    "seq_size = 35\n",
    "batch_size = 20\n",
    "embedding_size =650\n",
    "lstm_size = 650\n",
    "num_layers = 2\n",
    "gradients_norm = 5\n",
    "top_k = 5\n",
    "epochs = 50\n",
    "\n",
    "# For optimizer\n",
    "lr = 0.001\n",
    "weight_decay = 0.00002 # L2 regularization\n",
    "\n",
    "\n",
    "#Dropouts\n",
    "dropout=0.5 #locked\n",
    "dropouti=0.5 #locked\n",
    "dropoute=0.1 #emb dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTwHWsbSWuO3"
   },
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "avLFVK0nvuTe",
    "outputId": "9f96e92c-f039-428e-84f4-5c57d0f11d71"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f48b6ff4-0b05-40d2-b4b5-27024ac52f08\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-f48b6ff4-0b05-40d2-b4b5-27024ac52f08\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving KP.txt to KP.txt\n",
      "Vocabulary size 6433\n",
      "Total size 56445\n",
      "Training size 45156\n",
      "Validation size 11289\n"
     ]
    }
   ],
   "source": [
    "# Kim Possible Data\n",
    "\n",
    "import re\n",
    "\n",
    "#from google.colab import files\n",
    "#train_file = files.upload()\n",
    "\n",
    "\n",
    "train_file = 'KP.txt'\n",
    "checkpoint_path = 'checkpoint'\n",
    "\n",
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file,'r',encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        pat = re.compile(r\"([.()!,])\")\n",
    "        text = pat.sub(\" \\\\1 \", text)\n",
    "        text = text.replace(',', ' ')\n",
    "        text = text.lower().split()\n",
    "      \n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    print('Total size', len(int_text))\n",
    "\n",
    "    #TRAINING SET\n",
    "\n",
    "    int_text0 = int_text[:int(len(int_text)*0.8)]\n",
    "    num_batches = int(len(int_text0) / (seq_size * batch_size))\n",
    "    in_text = int_text0[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    print('Training size', len(int_text0))\n",
    "\n",
    "    #VALIDATION SET\n",
    "\n",
    "    int_text1 = int_text[int(len(int_text)*0.8):]\n",
    "    num_batches = int(len(int_text1) / (seq_size * batch_size))\n",
    "    in_text1 = int_text1[:num_batches * batch_size * seq_size]\n",
    "    out_text1 = np.zeros_like(in_text1)\n",
    "    out_text1[:-1] = in_text1[1:]\n",
    "    out_text1[-1] = in_text1[0]\n",
    "    in_text_val = np.reshape(in_text1, (batch_size, -1))\n",
    "    out_text_val = np.reshape(out_text1, (batch_size, -1))\n",
    "    print('Validation size', len(int_text1))\n",
    "\n",
    "                              \n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val\n",
    "\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val= get_data_from_file(train_file, batch_size, seq_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BD6ZpZ9kW1MP"
   },
   "source": [
    "## Define class and functions for regularization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0T07ZeWcfgO"
   },
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return x\n",
    "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
    "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2k81eF6c0vf"
   },
   "outputs": [],
   "source": [
    "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
    "  if dropout:\n",
    "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout) \n",
    "    masked_embed_weight = mask * embed.weight\n",
    "  else:\n",
    "    masked_embed_weight = embed.weight\n",
    "    \n",
    "  padding_idx = embed.padding_idx\n",
    "  if padding_idx is None:\n",
    "      padding_idx = -1\n",
    "\n",
    "  X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "    padding_idx, embed.max_norm, embed.norm_type,\n",
    "    embed.scale_grad_by_freq, embed.sparse\n",
    "  )\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIKZiheBW7po"
   },
   "source": [
    "## Define model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FViF_Gx1bcAy"
   },
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, seq_size, \n",
    "                 embedding_size, lstm_size, num_layers, dropout=0.5, dropouti=0.5, dropoute=0.1):\n",
    "        super(RNNModule, self).__init__()\n",
    "       \n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lockdrop = LockedDropout()\n",
    "        self.idrop = nn.Dropout(dropouti)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_size, \n",
    "                            num_layers, batch_first=True)\n",
    "      \n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.dropouti = dropouti\n",
    "        self.dropoute = dropoute\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.bias.data.fill_(0)\n",
    "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = embedded_dropout(self.embedding, x, dropout=self.dropoute if self.training else 0)\n",
    "        embed = self.idrop(embed)\n",
    "        embed = self.lockdrop(embed, self.dropouti)\n",
    "\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "\n",
    "        output = self.lockdrop(output, self.dropout)\n",
    "\n",
    "        output = self.dense(output)\n",
    "\n",
    "        return output, state, embed\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, \n",
    "                            self.lstm_size),\n",
    "                torch.zeros(self.num_layers, batch_size, \n",
    "                            self.lstm_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwwoIBzcw-ov"
   },
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr, weight_decay):   \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr, weight_decay=weight_decay)\n",
    "    return criterion, optimizer\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4Zpv2EfmlSz"
   },
   "source": [
    "## Training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yJ5_cCKgx6nH",
    "outputId": "52ac0af3-9abd-465a-8227-58a870d45d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time: 0m 1s Epoch: 1/50 Training loss: 6.789502665400505 Validation loss: 8.767270028591156 Validation perplexity: 6420.626974698962\n",
      "\n",
      "\n",
      "Time: 0m 3s Epoch: 2/50 Training loss: 6.232224680483341 Validation loss: 6.464867949485779 Validation perplexity: 643.9390964041859\n",
      "\n",
      "\n",
      "Time: 0m 4s Epoch: 3/50 Training loss: 6.0263165310025215 Validation loss: 6.302557498216629 Validation perplexity: 547.947536122507\n",
      "\n",
      "\n",
      "Time: 0m 5s Epoch: 4/50 Training loss: 5.8422074764966965 Validation loss: 6.168674051761627 Validation perplexity: 479.51016575791135\n",
      "\n",
      "\n",
      "Time: 0m 7s Epoch: 5/50 Training loss: 5.723537869751453 Validation loss: 6.079588204622269 Validation perplexity: 439.03457318990996\n",
      "\n",
      "\n",
      "Time: 0m 8s Epoch: 6/50 Training loss: 5.613759145140648 Validation loss: 6.035757303237915 Validation perplexity: 420.4203177305129\n",
      "\n",
      "\n",
      "Time: 0m 10s Epoch: 7/50 Training loss: 5.509028509259224 Validation loss: 5.993395090103149 Validation perplexity: 403.04496033775825\n",
      "\n",
      "\n",
      "Time: 0m 11s Epoch: 8/50 Training loss: 5.396863028407097 Validation loss: 5.948347985744476 Validation perplexity: 385.60635918257077\n",
      "\n",
      "\n",
      "Time: 0m 13s Epoch: 9/50 Training loss: 5.323496326804161 Validation loss: 5.930022567510605 Validation perplexity: 378.7387545938297\n",
      "\n",
      "\n",
      "Time: 0m 14s Epoch: 10/50 Training loss: 5.22458066791296 Validation loss: 5.9049378633499146 Validation perplexity: 369.18672034335725\n",
      "\n",
      "\n",
      "Time: 0m 16s Epoch: 11/50 Training loss: 5.147617444396019 Validation loss: 5.907853811979294 Validation perplexity: 370.398756926985\n",
      "\n",
      "\n",
      "Time: 0m 17s Epoch: 12/50 Training loss: 5.059774778783321 Validation loss: 5.879009515047073 Validation perplexity: 360.0955033630093\n",
      "\n",
      "\n",
      "Time: 0m 19s Epoch: 13/50 Training loss: 4.9981280118227005 Validation loss: 5.874871701002121 Validation perplexity: 358.75523458077873\n",
      "\n",
      "\n",
      "Time: 0m 20s Epoch: 14/50 Training loss: 4.905948922038078 Validation loss: 5.8662673234939575 Validation perplexity: 355.6926800336754\n",
      "\n",
      "\n",
      "Time: 0m 22s Epoch: 15/50 Training loss: 4.825689367949963 Validation loss: 5.885075151920319 Validation perplexity: 362.6637803033353\n",
      "\n",
      "\n",
      "Time: 0m 23s Epoch: 16/50 Training loss: 4.72418362647295 Validation loss: 5.883141994476318 Validation perplexity: 362.0862498728509\n",
      "\n",
      "\n",
      "Time: 0m 25s Epoch: 17/50 Training loss: 4.629666410386562 Validation loss: 5.8935962319374084 Validation perplexity: 365.9913568176288\n",
      "\n",
      "\n",
      "Time: 0m 26s Epoch: 18/50 Training loss: 4.598861820995808 Validation loss: 5.898064911365509 Validation perplexity: 367.9817786107606\n",
      "\n",
      "\n",
      "Time: 0m 27s Epoch: 19/50 Training loss: 4.475468628108501 Validation loss: 5.907926589250565 Validation perplexity: 371.6974158104871\n",
      "\n",
      "\n",
      "Time: 0m 29s Epoch: 20/50 Training loss: 4.414915084838867 Validation loss: 5.9172724187374115 Validation perplexity: 375.17042425915145\n",
      "\n",
      "\n",
      "Time: 0m 30s Epoch: 21/50 Training loss: 4.33849335834384 Validation loss: 5.9182062447071075 Validation perplexity: 375.2334273488572\n",
      "\n",
      "\n",
      "Time: 0m 32s Epoch: 22/50 Training loss: 4.24966025352478 Validation loss: 5.945212125778198 Validation perplexity: 385.7488290693635\n",
      "\n",
      "\n",
      "Time: 0m 33s Epoch: 23/50 Training loss: 4.138467378914356 Validation loss: 5.942917734384537 Validation perplexity: 385.1673201383762\n",
      "\n",
      "\n",
      "Time: 0m 35s Epoch: 24/50 Training loss: 4.0323064140975475 Validation loss: 5.954596132040024 Validation perplexity: 389.6156247508115\n",
      "\n",
      "\n",
      "Time: 0m 36s Epoch: 25/50 Training loss: 4.011132586747408 Validation loss: 5.959917962551117 Validation perplexity: 391.9892538917686\n",
      "\n",
      "\n",
      "Time: 0m 38s Epoch: 26/50 Training loss: 3.956579592078924 Validation loss: 5.967930853366852 Validation perplexity: 395.2237626581212\n",
      "\n",
      "\n",
      "Time: 0m 39s Epoch: 27/50 Training loss: 3.8507919758558273 Validation loss: 6.011501252651215 Validation perplexity: 412.8502334784952\n",
      "\n",
      "\n",
      "Time: 0m 41s Epoch: 28/50 Training loss: 3.7509302236139774 Validation loss: 6.004464745521545 Validation perplexity: 410.02625840516436\n",
      "\n",
      "\n",
      "Time: 0m 42s Epoch: 29/50 Training loss: 3.7111268304288387 Validation loss: 6.041035741567612 Validation perplexity: 424.97010147248926\n",
      "\n",
      "\n",
      "Time: 0m 44s Epoch: 30/50 Training loss: 3.5879022404551506 Validation loss: 6.044371247291565 Validation perplexity: 426.6269558786625\n",
      "\n",
      "\n",
      "Time: 0m 45s Epoch: 31/50 Training loss: 3.5581519305706024 Validation loss: 6.05854070186615 Validation perplexity: 433.1527934924966\n",
      "\n",
      "\n",
      "Time: 0m 47s Epoch: 32/50 Training loss: 3.452872771769762 Validation loss: 6.067999839782715 Validation perplexity: 437.2997656719833\n",
      "\n",
      "\n",
      "Time: 0m 48s Epoch: 33/50 Training loss: 3.4111603386700153 Validation loss: 6.07792255282402 Validation perplexity: 441.81741958401994\n",
      "\n",
      "\n",
      "Time: 0m 50s Epoch: 34/50 Training loss: 3.3793830648064613 Validation loss: 6.092485576868057 Validation perplexity: 448.07502252310235\n",
      "\n",
      "\n",
      "Time: 0m 51s Epoch: 35/50 Training loss: 3.259228825569153 Validation loss: 6.100210189819336 Validation perplexity: 451.91408150644475\n",
      "\n",
      "\n",
      "Time: 0m 53s Epoch: 36/50 Training loss: 3.1856294833123684 Validation loss: 6.1181067526340485 Validation perplexity: 460.04785814951697\n",
      "\n",
      "\n",
      "Time: 0m 54s Epoch: 37/50 Training loss: 3.129870131611824 Validation loss: 6.137000381946564 Validation perplexity: 468.7443137772062\n",
      "\n",
      "\n",
      "Time: 0m 55s Epoch: 38/50 Training loss: 3.0965431816875935 Validation loss: 6.130116969347 Validation perplexity: 465.8441441902138\n",
      "\n",
      "\n",
      "Time: 0m 57s Epoch: 39/50 Training loss: 2.993247114121914 Validation loss: 6.151926577091217 Validation perplexity: 475.5779834436397\n",
      "\n",
      "\n",
      "Time: 0m 58s Epoch: 40/50 Training loss: 3.038802534341812 Validation loss: 6.169207870960236 Validation perplexity: 483.8660070781577\n",
      "\n",
      "\n",
      "Time: 1m 0s Epoch: 41/50 Training loss: 2.97613762319088 Validation loss: 6.161913305521011 Validation perplexity: 480.45666835827956\n",
      "\n",
      "\n",
      "Time: 1m 1s Epoch: 42/50 Training loss: 2.855691086500883 Validation loss: 6.19087889790535 Validation perplexity: 495.353989748595\n",
      "\n",
      "\n",
      "Time: 1m 3s Epoch: 43/50 Training loss: 2.8438264578580856 Validation loss: 6.188376754522324 Validation perplexity: 493.74322396821714\n",
      "\n",
      "\n",
      "Time: 1m 4s Epoch: 44/50 Training loss: 2.795573092997074 Validation loss: 6.206670820713043 Validation perplexity: 503.04694487630917\n",
      "\n",
      "\n",
      "Time: 1m 6s Epoch: 45/50 Training loss: 2.775168403983116 Validation loss: 6.219871878623962 Validation perplexity: 509.19097615189105\n",
      "\n",
      "\n",
      "Time: 1m 7s Epoch: 46/50 Training loss: 2.7050672695040703 Validation loss: 6.226562529802322 Validation perplexity: 512.6827213895676\n",
      "\n",
      "\n",
      "Time: 1m 9s Epoch: 47/50 Training loss: 2.570796299725771 Validation loss: 6.248144090175629 Validation perplexity: 523.7171358288803\n",
      "\n",
      "\n",
      "Time: 1m 10s Epoch: 48/50 Training loss: 2.4736312180757523 Validation loss: 6.249794661998749 Validation perplexity: 524.5765921527247\n",
      "\n",
      "\n",
      "Time: 1m 12s Epoch: 49/50 Training loss: 2.496887259185314 Validation loss: 6.282813996076584 Validation perplexity: 543.1198505959677\n",
      "\n",
      "\n",
      "Time: 1m 13s Epoch: 50/50 Training loss: 2.4835821129381657 Validation loss: 6.295206606388092 Validation perplexity: 548.6959476688561\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Initialize a new network\n",
    "net = RNNModule(n_vocab, seq_size, embedding_size, lstm_size, num_layers,dropout, dropouti, dropoute)\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "criterion, optimizer = get_loss_and_train_op(net, lr, weight_decay=weight_decay)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "training_loss, validation_loss, validation_perplex = [], [], []\n",
    "\n",
    "# For each epoch\n",
    "for e in range(1,epochs+1):\n",
    "  \n",
    "    losses_train, losses_val, perplexity_val = [], [], []\n",
    "\n",
    "    batches_val = get_batches(in_text_val, out_text_val, batch_size, seq_size)\n",
    "    state_h, state_c = net.zero_state(batch_size)\n",
    "\n",
    "    # Transfer data to GPU\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "      \n",
    "    # For each sentence in validation set\n",
    "    for x,y in batches_val:\n",
    "                  \n",
    "        # Tell it we are in eval mode\n",
    "        net.eval()\n",
    "\n",
    "        # Make tensors\n",
    "        x = torch.LongTensor(x).to(device) # inputs\n",
    "        y = torch.LongTensor(y).to(device) # targets\n",
    "          \n",
    "        #Forward pass \n",
    "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
    "        loss = criterion(outputs.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        # Compute loss and perplexity\n",
    "        loss_value = loss.item()\n",
    "        losses_val.append(loss_value)\n",
    "        perplex = math.exp(loss_value)\n",
    "        perplexity_val.append(perplex)\n",
    "      \n",
    "    batches = get_batches(in_text, out_text, batch_size,seq_size)\n",
    "    state_h, state_c = net.zero_state(batch_size)\n",
    "    \n",
    "    # Transfer data to GPU\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    # For each sentence in training set\n",
    "    for x,y in batches:\n",
    "        \n",
    "        # Tell it we are in training mode\n",
    "        net.train()\n",
    "        \n",
    "        # Reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make tensors\n",
    "        x = torch.LongTensor(x).to(device) # inputs\n",
    "        y = torch.LongTensor(y).to(device) # targets\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
    "        loss = criterion(outputs.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        losses_train.append(loss_value)\n",
    "        \n",
    "        # Perform back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
    "\n",
    "        # Update the network's parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Save loss and perplexity for plot\n",
    "    training_loss.append(np.mean(losses_train))\n",
    "    validation_loss.append(np.mean(losses_val))\n",
    "    validation_perplex.append(np.mean(perplexity_val))\n",
    "     \n",
    "    # Print at every epoch    \n",
    "    if e % 1 == 0:\n",
    "        print('\\n') \n",
    "        print('Time: {}'.format(time_since(start)),\n",
    "              'Epoch: {}/{}'.format(e, epochs),\n",
    "              'Training loss: {}'.format(training_loss[-1]),\n",
    "              'Validation loss: {}'.format(validation_loss[-1]),\n",
    "              'Validation perplexity: {}'.format(validation_perplex[-1]))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORArxLEimswD"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "*Fixed k*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QrQaaQTw-o8"
   },
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k, out_size):\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    \n",
    "    for w in words:\n",
    "        ix = torch.LongTensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "    words.append(int_to_vocab[choice])\n",
    "\n",
    "    for _ in range(out_size):\n",
    "        ix = torch.LongTensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c), _= net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "    print('\\n')    \n",
    "    \n",
    "    out = ' '.join(words)\n",
    "    out = out.replace(' .','.')\n",
    "    out = '. '.join(i.capitalize() for i in out.split(\". \"))\n",
    "    print(out) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "HF4S0EultBBQ",
    "outputId": "181880a6-9cd4-4ad3-c9ac-94b55255d3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Kim says she's surprised but ron refuses the call on the other hand as a distraction and is relieved to get the magma. The monkey ninjas after ron and the twins see kim at ron but kim pulls up a humiliation baseball\n"
     ]
    }
   ],
   "source": [
    "predict(device,net,['kim', 'says'], n_vocab, vocab_to_int, int_to_vocab, 5, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsb8q4IJw-pU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KimP_THE BEST MODEL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
