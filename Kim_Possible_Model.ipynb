{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KimP_THE BEST MODEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NPRug3HOw-oX"
      },
      "source": [
        "## LSTM Language Model for Kim Possible Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJASEzg7w-oY",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import chainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import re\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Hyperparameters\n",
        "seq_size = 35\n",
        "batch_size = 20\n",
        "embedding_size =650\n",
        "lstm_size = 650\n",
        "num_layers = 2\n",
        "gradients_norm = 5\n",
        "epochs = 20\n",
        "\n",
        "# For optimizer\n",
        "lr = 0.001\n",
        "weight_decay = 2e-5 # L2 regularization\n",
        "\n",
        "\n",
        "#Dropouts\n",
        "dropout=0.5 #locked\n",
        "dropouti=0.5 #locked\n",
        "dropoute=0.1 #emb dropout\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTwHWsbSWuO3",
        "colab_type": "text"
      },
      "source": [
        "## Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avLFVK0nvuTe",
        "colab_type": "code",
        "outputId": "270c6af8-86b2-4d03-dc99-82ed1afc9c65",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Kim Possible Data\n",
        "\n",
        "import re\n",
        "\n",
        "from google.colab import files\n",
        "train_file = files.upload()\n",
        "\n",
        "\n",
        "train_file = 'KP.txt'\n",
        "checkpoint_path = 'checkpoint'\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file,'r',encoding=\"utf8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "        pat = re.compile(r\"([.()!,])\")\n",
        "        text = pat.sub(\" \\\\1 \", text)\n",
        "        text = text.replace(',', ' ')\n",
        "        text = text.lower().split()\n",
        "\n",
        "    names = ['kim','ron','wade','rufus','shego','drakken','lucre']\n",
        "\n",
        "    for i in range(len(text)):\n",
        "      if text[i] in names:\n",
        "        text[i] = text[i].capitalize() \n",
        "  \n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "\n",
        "    n_vocab = len(int_to_vocab)\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    print('Total size', len(int_text))\n",
        "\n",
        "    #TRAINING SET\n",
        "\n",
        "    int_text0 = int_text[:int(len(int_text)*0.8)]\n",
        "    num_batches = int(len(int_text0) / (seq_size * batch_size))\n",
        "    in_text = int_text0[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    print('Training size', len(int_text0))\n",
        "\n",
        "    #VALIDATION SET\n",
        "\n",
        "    int_text1 = int_text[int(len(int_text)*0.8):]\n",
        "    num_batches = int(len(int_text1) / (seq_size * batch_size))\n",
        "    in_text1 = int_text1[:num_batches * batch_size * seq_size]\n",
        "    out_text1 = np.zeros_like(in_text1)\n",
        "    out_text1[:-1] = in_text1[1:]\n",
        "    out_text1[-1] = in_text1[0]\n",
        "    in_text_val = np.reshape(in_text1, (batch_size, -1))\n",
        "    out_text_val = np.reshape(out_text1, (batch_size, -1))\n",
        "    print('Validation size', len(int_text1))\n",
        "\n",
        "                              \n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val\n",
        "\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val= get_data_from_file(train_file, batch_size, seq_size)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fe6ce59c-b3b0-493f-a7c4-2696a51913a0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-fe6ce59c-b3b0-493f-a7c4-2696a51913a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving KP.txt to KP (7).txt\n",
            "Vocabulary size 6433\n",
            "Total size 56445\n",
            "Training size 45156\n",
            "Validation size 11289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD6ZpZ9kW1MP",
        "colab_type": "text"
      },
      "source": [
        "## Define regularization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0T07ZeWcfgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2k81eF6c0vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
        "  if dropout:\n",
        "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout) \n",
        "    masked_embed_weight = mask * embed.weight\n",
        "  else:\n",
        "    masked_embed_weight = embed.weight\n",
        "    \n",
        "  padding_idx = embed.padding_idx\n",
        "  if padding_idx is None:\n",
        "      padding_idx = -1\n",
        "\n",
        "  X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
        "    padding_idx, embed.max_norm, embed.norm_type,\n",
        "    embed.scale_grad_by_freq, embed.sparse\n",
        "  )\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIKZiheBW7po",
        "colab_type": "text"
      },
      "source": [
        "## Define model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FViF_Gx1bcAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_vocab, seq_size, \n",
        "                 embedding_size, lstm_size, num_layers, dropout=0.5, dropouti=0.5, dropoute=0.1):\n",
        "        super(RNNModule, self).__init__()\n",
        "       \n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lockdrop = LockedDropout()\n",
        "        self.idrop = nn.Dropout(dropouti)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_size, \n",
        "                            num_layers, batch_first=True)\n",
        "      \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.dropouti = dropouti\n",
        "        self.dropoute = dropoute\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.dense.bias.data.fill_(0)\n",
        "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def forward(self, x, prev_state):\n",
        "        embed = embedded_dropout(self.embedding, x, dropout=self.dropoute if self.training else 0)\n",
        "        embed = self.idrop(embed)\n",
        "        embed = self.lockdrop(embed, self.dropouti)\n",
        "\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        output = self.lockdrop(output, self.dropout)\n",
        "\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output, state, embed\n",
        "    \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, \n",
        "                            self.lstm_size),\n",
        "                torch.zeros(self.num_layers, batch_size, \n",
        "                            self.lstm_size))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qwwoIBzcw-ov",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lr, weight_decay):   \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr, weight_decay=weight_decay)\n",
        "    return criterion, optimizer\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Zpv2EfmlSz",
        "colab_type": "text"
      },
      "source": [
        "## Training and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ5_cCKgx6nH",
        "outputId": "4c29e981-6270-42d4-fc19-f009ba303173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TRAINING\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Initialize a new network\n",
        "net = RNNModule(n_vocab, seq_size, embedding_size, lstm_size, num_layers,dropout, dropouti, dropoute)\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "criterion, optimizer = get_loss_and_train_op(net, lr, weight_decay=weight_decay)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "training_loss, validation_loss, validation_perplex = [], [], []\n",
        "\n",
        "# For each epoch\n",
        "for e in range(1,epochs+1):\n",
        "  \n",
        "    losses_train, losses_val, perplexity_val = [], [], []\n",
        "\n",
        "    batches_val = get_batches(in_text_val, out_text_val, batch_size, seq_size)\n",
        "    state_h, state_c = net.zero_state(batch_size)\n",
        "\n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "      \n",
        "    # For each sentence in validation set\n",
        "    for x,y in batches_val:\n",
        "                  \n",
        "        # Tell it we are in eval mode\n",
        "        net.eval()\n",
        "\n",
        "        # Make tensors\n",
        "        x = torch.LongTensor(x).to(device) # inputs\n",
        "        y = torch.LongTensor(y).to(device) # targets\n",
        "          \n",
        "        #Forward pass \n",
        "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
        "        loss = criterion(outputs.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "\n",
        "        # Compute loss and perplexity\n",
        "        loss_value = loss.item()\n",
        "        losses_val.append(loss_value)\n",
        "        perplex = math.exp(loss_value)\n",
        "        perplexity_val.append(perplex)\n",
        "      \n",
        "    batches = get_batches(in_text, out_text, batch_size,seq_size)\n",
        "    state_h, state_c = net.zero_state(batch_size)\n",
        "    \n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    # For each sentence in training set\n",
        "    for x,y in batches:\n",
        "        \n",
        "        # Tell it we are in training mode\n",
        "        net.train()\n",
        "        \n",
        "        # Reset all gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Make tensors\n",
        "        x = torch.LongTensor(x).to(device) # inputs\n",
        "        y = torch.LongTensor(y).to(device) # targets\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
        "        loss = criterion(outputs.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "        \n",
        "        loss_value = loss.item()\n",
        "        losses_train.append(loss_value)\n",
        "        \n",
        "        # Perform back-propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
        "\n",
        "        # Update the network's parameters\n",
        "        optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Save loss and perplexity for plot\n",
        "    training_loss.append(np.mean(losses_train))\n",
        "    validation_loss.append(np.mean(losses_val))\n",
        "    validation_perplex.append(np.mean(perplexity_val))\n",
        "     \n",
        "    # Print at every epoch    \n",
        "    if e % 1 == 0:\n",
        "        print('\\n') \n",
        "        print('Time: {}'.format(time_since(start)),\n",
        "              'Epoch: {}/{}'.format(e, epochs),\n",
        "              'Training loss: {}'.format(training_loss[-1]),\n",
        "              'Validation loss: {}'.format(validation_loss[-1]),\n",
        "              'Validation perplexity: {}'.format(validation_perplex[-1]))\n",
        "   "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Time: 0m 1s Epoch: 1/20 Training loss: 6.795324586331844 Validation loss: 8.771742105484009 Validation perplexity: 6449.404748565164\n",
            "\n",
            "\n",
            "Time: 0m 2s Epoch: 2/20 Training loss: 6.241818882524967 Validation loss: 6.46930205821991 Validation perplexity: 646.8580399997549\n",
            "\n",
            "\n",
            "Time: 0m 4s Epoch: 3/20 Training loss: 6.049874819815159 Validation loss: 6.327137261629105 Validation perplexity: 561.4608976680363\n",
            "\n",
            "\n",
            "Time: 0m 5s Epoch: 4/20 Training loss: 5.871554523706436 Validation loss: 6.186087518930435 Validation perplexity: 487.8051939705608\n",
            "\n",
            "\n",
            "Time: 0m 7s Epoch: 5/20 Training loss: 5.738460153341293 Validation loss: 6.102333664894104 Validation perplexity: 448.78749464705487\n",
            "\n",
            "\n",
            "Time: 0m 8s Epoch: 6/20 Training loss: 5.638442896306515 Validation loss: 6.031789481639862 Validation perplexity: 418.3534267332406\n",
            "\n",
            "\n",
            "Time: 0m 9s Epoch: 7/20 Training loss: 5.54202139377594 Validation loss: 5.991869390010834 Validation perplexity: 402.3048838968772\n",
            "\n",
            "\n",
            "Time: 0m 11s Epoch: 8/20 Training loss: 5.445445142686367 Validation loss: 5.954699009656906 Validation perplexity: 387.6841946287276\n",
            "\n",
            "\n",
            "Time: 0m 12s Epoch: 9/20 Training loss: 5.354049049317837 Validation loss: 5.929382890462875 Validation perplexity: 378.1444863055366\n",
            "\n",
            "\n",
            "Time: 0m 14s Epoch: 10/20 Training loss: 5.2347875982522964 Validation loss: 5.926532834768295 Validation perplexity: 377.35258501439296\n",
            "\n",
            "\n",
            "Time: 0m 15s Epoch: 11/20 Training loss: 5.210226409137249 Validation loss: 5.8956621289253235 Validation perplexity: 365.9778379268798\n",
            "\n",
            "\n",
            "Time: 0m 16s Epoch: 12/20 Training loss: 5.10511527210474 Validation loss: 5.90797546505928 Validation perplexity: 370.442752424459\n",
            "\n",
            "\n",
            "Time: 0m 18s Epoch: 13/20 Training loss: 5.021196611225605 Validation loss: 5.891161233186722 Validation perplexity: 364.5054321547017\n",
            "\n",
            "\n",
            "Time: 0m 19s Epoch: 14/20 Training loss: 4.959149159491062 Validation loss: 5.887256532907486 Validation perplexity: 363.15420714123746\n",
            "\n",
            "\n",
            "Time: 0m 21s Epoch: 15/20 Training loss: 4.872279621660709 Validation loss: 5.881246715784073 Validation perplexity: 360.9119228657653\n",
            "\n",
            "\n",
            "Time: 0m 22s Epoch: 16/20 Training loss: 4.775581829249859 Validation loss: 5.891483038663864 Validation perplexity: 364.7024595135097\n",
            "\n",
            "\n",
            "Time: 0m 23s Epoch: 17/20 Training loss: 4.7348043248057365 Validation loss: 5.888634502887726 Validation perplexity: 363.8413665409039\n",
            "\n",
            "\n",
            "Time: 0m 25s Epoch: 18/20 Training loss: 4.628617763519287 Validation loss: 5.894094914197922 Validation perplexity: 365.6725738332668\n",
            "\n",
            "\n",
            "Time: 0m 26s Epoch: 19/20 Training loss: 4.550999090075493 Validation loss: 5.905611336231232 Validation perplexity: 370.15372568594023\n",
            "\n",
            "\n",
            "Time: 0m 28s Epoch: 20/20 Training loss: 4.488798685371876 Validation loss: 5.896580725908279 Validation perplexity: 366.9188354177654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORArxLEimswD",
        "colab_type": "text"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "*Fixed k*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QrQaaQTw-o8",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k, out_size):\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    \n",
        "    for w in words:\n",
        "        ix = torch.LongTensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(out_size):\n",
        "        ix = torch.LongTensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c), _= net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "    print('\\n')    \n",
        "    \n",
        "    \n",
        "    for i in range(len(words)-1):\n",
        "      if i == 0:\n",
        "        words[i] = words[i].capitalize()\n",
        "      if words[i] == '.':\n",
        "        words[i+1] =words[i+1].capitalize()\n",
        "    \n",
        "    out = ' '.join(words)\n",
        "    out = out.replace(' .','.')\n",
        "\n",
        "    print(out)\n",
        "\n",
        "    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4S0EultBBQ",
        "colab_type": "code",
        "outputId": "8bfa7d65-47fe-4011-bee4-ef3b9122be3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "predict(device,net,['Kim', 'is'], n_vocab, vocab_to_int, int_to_vocab, 5, 100)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Kim is happy to be seen for a good and Ron remarks to her and then asks if he knows her. Ron tells them a job on a smarty new boy. Kim says it's the idea to find her to get a cabin and she finds the data from the room to the ground. Kim is concerned to get the idea and tries out that it was the same world. He is able on the kimmunicator but Wade is immediately happy. The sales day the two and Kim asks Ron if she knows it to know him as\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}