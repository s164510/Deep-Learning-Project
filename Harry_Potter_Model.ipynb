{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Harry_Potter_THE BEST MODEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NPRug3HOw-oX"
      },
      "source": [
        "## LSTM Language Model for Harry Potter (three books)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJASEzg7w-oY",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import chainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import re\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Hyperparameters\n",
        "seq_size = 35\n",
        "batch_size = 20\n",
        "embedding_size =650\n",
        "lstm_size = 650\n",
        "num_layers = 2\n",
        "gradients_norm = 5\n",
        "epochs = 25\n",
        "\n",
        "# For optimizer\n",
        "lr = 0.001\n",
        "weight_decay = 2e-5 # L2 regularization\n",
        "\n",
        "\n",
        "#Dropouts\n",
        "dropout=0.5 #locked\n",
        "dropouti=0.5 #locked\n",
        "dropoute=0.1 #emb dropout\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN9ENWQvbXx1",
        "colab_type": "text"
      },
      "source": [
        "## Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avLFVK0nvuTe",
        "colab_type": "code",
        "outputId": "31f7a06d-f5a9-4789-efae-9f71f9be79ac",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Harry Potter Data\n",
        "\n",
        "from google.colab import files\n",
        "train_file = files.upload()\n",
        "\n",
        "\n",
        "train_file = 'HP.txt'\n",
        "checkpoint_path = 'checkpoint'\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file,'r',encoding=\"utf8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "#        text = text[:int(len(text))]\n",
        "        pat = re.compile(r\"([.()!])\")\n",
        "        text = pat.sub(\" \\\\1 \", text)\n",
        "        text = text.replace('\"','\" ')\n",
        "        text = text.lower().split()\n",
        "      \n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    print('Total size', len(int_text))\n",
        "\n",
        "    #TRAINING SET\n",
        "\n",
        "    int_text0 = int_text[:int(len(int_text)*0.8)]\n",
        "    num_batches = int(len(int_text0) / (seq_size * batch_size))\n",
        "    in_text = int_text0[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    print('Training size', len(int_text0))\n",
        "\n",
        "    #VALIDATION SET\n",
        "\n",
        "    int_text1 = int_text[int(len(int_text)*0.8):]\n",
        "    num_batches = int(len(int_text1) / (seq_size * batch_size))\n",
        "    in_text1 = int_text1[:num_batches * batch_size * seq_size]\n",
        "    out_text1 = np.zeros_like(in_text1)\n",
        "    out_text1[:-1] = in_text1[1:]\n",
        "    out_text1[-1] = in_text1[0]\n",
        "    in_text_val = np.reshape(in_text1, (batch_size, -1))\n",
        "    out_text_val = np.reshape(out_text1, (batch_size, -1))\n",
        "    print('Validation size', len(int_text1))\n",
        "\n",
        "                              \n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val\n",
        "\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val= get_data_from_file(train_file, batch_size, seq_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c48467d8-495c-4cf9-8463-b4cfd3c314a9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c48467d8-495c-4cf9-8463-b4cfd3c314a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving HP.txt to HP (1).txt\n",
            "Vocabulary size 16957\n",
            "Total size 255819\n",
            "Training size 204655\n",
            "Validation size 51164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5RpMeSXbbNG",
        "colab_type": "text"
      },
      "source": [
        "## Define regularization functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0T07ZeWcfgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2k81eF6c0vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
        "  if dropout:\n",
        "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n",
        "    masked_embed_weight = mask * embed.weight\n",
        "  else:\n",
        "    masked_embed_weight = embed.weight\n",
        "\n",
        "  padding_idx = embed.padding_idx\n",
        "  if padding_idx is None:\n",
        "      padding_idx = -1\n",
        "\n",
        "  X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
        "    padding_idx, embed.max_norm, embed.norm_type,\n",
        "    embed.scale_grad_by_freq, embed.sparse\n",
        "  )\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV_0rnWRbhsW",
        "colab_type": "text"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FViF_Gx1bcAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_vocab, seq_size, \n",
        "                 embedding_size, lstm_size, num_layers, dropout=0.5, dropouti=0.5, dropoute=0.1):\n",
        "        super(RNNModule, self).__init__()\n",
        "       \n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lockdrop = LockedDropout()\n",
        "        self.idrop = nn.Dropout(dropouti)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_size, \n",
        "                            num_layers, batch_first=True)\n",
        "      \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.dropouti = dropouti\n",
        "        self.dropoute = dropoute\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.dense.bias.data.fill_(0)\n",
        "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def forward(self, x, prev_state):\n",
        "        embed = embedded_dropout(self.embedding, x, dropout=self.dropoute if self.training else 0)\n",
        "        embed = self.idrop(embed)\n",
        "        embed = self.lockdrop(embed, self.dropouti)\n",
        "\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        output = self.lockdrop(output, self.dropout)\n",
        "\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output, state, embed\n",
        "    \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, \n",
        "                            self.lstm_size),\n",
        "                torch.zeros(self.num_layers, batch_size, \n",
        "                            self.lstm_size))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qwwoIBzcw-ov",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lr, weight_decay=weight_decay):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr, weight_decay=weight_decay)\n",
        "    return criterion, optimizer\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Zpv2EfmlSz",
        "colab_type": "text"
      },
      "source": [
        "## Training and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ5_cCKgx6nH",
        "outputId": "aa284b10-ce0d-458c-c3dd-b8afabd617d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TRAINING\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Initialize a new network\n",
        "net = RNNModule(n_vocab, seq_size, embedding_size, lstm_size, num_layers,dropout, dropouti, dropoute)\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "criterion, optimizer = get_loss_and_train_op(net, lr,weight_decay=weight_decay)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "training_loss, validation_loss, validation_perplex = [], [], []\n",
        "\n",
        "# For each epoch\n",
        "for e in range(1,epochs+1):\n",
        "  \n",
        "    losses_train, losses_val, perplexity_val = [], [], []\n",
        "\n",
        "    batches_val = get_batches(in_text_val, out_text_val, batch_size, seq_size)\n",
        "    state_h, state_c = net.zero_state(batch_size)\n",
        "\n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "      \n",
        "    # For each sentence in validation set\n",
        "    for x,y in batches_val:\n",
        "                  \n",
        "        # Tell it we are in eval mode\n",
        "        net.eval()\n",
        "\n",
        "        # Make tensors\n",
        "        x = torch.LongTensor(x).to(device) # inputs\n",
        "        y = torch.LongTensor(y).to(device) # targets\n",
        "          \n",
        "        #Forward pass \n",
        "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
        "        loss = criterion(outputs.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "\n",
        "        # Compute loss and perplexity\n",
        "        loss_value = loss.item()\n",
        "        losses_val.append(loss_value)\n",
        "        perplex = math.exp(loss_value)\n",
        "        perplexity_val.append(perplex)\n",
        "      \n",
        "    batches = get_batches(in_text, out_text, batch_size,seq_size)\n",
        "    state_h, state_c = net.zero_state(batch_size)\n",
        "    \n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    # For each sentence in training set\n",
        "    for x,y in batches:\n",
        "        \n",
        "        # Tell it we are in training mode\n",
        "        net.train()\n",
        "        \n",
        "        # Reset all gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Make tensors\n",
        "        x = torch.LongTensor(x).to(device) # inputs\n",
        "        y = torch.LongTensor(y).to(device) # targets\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
        "        loss = criterion(outputs.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "        \n",
        "        loss_value = loss.item()\n",
        "        losses_train.append(loss_value)\n",
        "        \n",
        "        # Perform back-propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
        "\n",
        "        # Update the network's parameters\n",
        "        optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Save loss and perplexity for plot\n",
        "    training_loss.append(np.mean(losses_train))\n",
        "    validation_loss.append(np.mean(losses_val))\n",
        "    validation_perplex.append(np.mean(perplexity_val))\n",
        "     \n",
        "    # Print at every epoch    \n",
        "    if e % 1 == 0:\n",
        "        print('\\n') \n",
        "        print('Time: {}'.format(time_since(start)),\n",
        "              'Epoch: {}/{}'.format(e, epochs),\n",
        "              'Training loss: {}'.format(training_loss[-1]),\n",
        "              'Validation loss: {}'.format(validation_loss[-1]),\n",
        "              'Validation perplexity: {}'.format(validation_perplex[-1]))\n",
        "   "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Time: 0m 9s Epoch: 1/25 Training loss: 6.666944428665997 Validation loss: 9.737840221352773 Validation perplexity: 16946.911855550417\n",
            "\n",
            "\n",
            "Time: 0m 19s Epoch: 2/25 Training loss: 6.115125937004612 Validation loss: 5.898343354055326 Validation perplexity: 368.7853502869146\n",
            "\n",
            "\n",
            "Time: 0m 29s Epoch: 3/25 Training loss: 5.871984978244729 Validation loss: 5.636886185162688 Validation perplexity: 284.3352185739361\n",
            "\n",
            "\n",
            "Time: 0m 39s Epoch: 4/25 Training loss: 5.694182965853443 Validation loss: 5.487579352235141 Validation perplexity: 245.1572553620241\n",
            "\n",
            "\n",
            "Time: 0m 49s Epoch: 5/25 Training loss: 5.551971442078891 Validation loss: 5.4112107655773425 Validation perplexity: 227.56645669768682\n",
            "\n",
            "\n",
            "Time: 0m 59s Epoch: 6/25 Training loss: 5.428916570258467 Validation loss: 5.342272105282301 Validation perplexity: 212.54380419776726\n",
            "\n",
            "\n",
            "Time: 1m 8s Epoch: 7/25 Training loss: 5.310706166371907 Validation loss: 5.288286176446366 Validation perplexity: 201.43516454447425\n",
            "\n",
            "\n",
            "Time: 1m 18s Epoch: 8/25 Training loss: 5.200053394657292 Validation loss: 5.2364292340735865 Validation perplexity: 191.24040653228016\n",
            "\n",
            "\n",
            "Time: 1m 28s Epoch: 9/25 Training loss: 5.090219265794101 Validation loss: 5.205222462954587 Validation perplexity: 185.530723017092\n",
            "\n",
            "\n",
            "Time: 1m 38s Epoch: 10/25 Training loss: 5.00042334321427 Validation loss: 5.159622290363051 Validation perplexity: 177.302018520713\n",
            "\n",
            "\n",
            "Time: 1m 48s Epoch: 11/25 Training loss: 4.910427184954082 Validation loss: 5.13992645315928 Validation perplexity: 174.01659652510378\n",
            "\n",
            "\n",
            "Time: 1m 58s Epoch: 12/25 Training loss: 4.804991191380645 Validation loss: 5.107946415470071 Validation perplexity: 168.49097577539987\n",
            "\n",
            "\n",
            "Time: 2m 8s Epoch: 13/25 Training loss: 4.7343917480886795 Validation loss: 5.079422656803915 Validation perplexity: 163.71367463780646\n",
            "\n",
            "\n",
            "Time: 2m 17s Epoch: 14/25 Training loss: 4.625609455043322 Validation loss: 5.072738608268843 Validation perplexity: 162.71883577888576\n",
            "\n",
            "\n",
            "Time: 2m 27s Epoch: 15/25 Training loss: 4.548064066939158 Validation loss: 5.062735668600422 Validation perplexity: 161.18983297860834\n",
            "\n",
            "\n",
            "Time: 2m 37s Epoch: 16/25 Training loss: 4.480391314584915 Validation loss: 5.05569997552323 Validation perplexity: 160.0375213323369\n",
            "\n",
            "\n",
            "Time: 2m 47s Epoch: 17/25 Training loss: 4.395231251847254 Validation loss: 5.026171436048534 Validation perplexity: 155.51333581606528\n",
            "\n",
            "\n",
            "Time: 2m 57s Epoch: 18/25 Training loss: 4.332754992458918 Validation loss: 5.002101003307185 Validation perplexity: 151.73803694000932\n",
            "\n",
            "\n",
            "Time: 3m 7s Epoch: 19/25 Training loss: 4.266182705147625 Validation loss: 5.008033014323614 Validation perplexity: 152.75178946368933\n",
            "\n",
            "\n",
            "Time: 3m 17s Epoch: 20/25 Training loss: 4.222288215813571 Validation loss: 5.015870146555443 Validation perplexity: 153.98138238871059\n",
            "\n",
            "\n",
            "Time: 3m 26s Epoch: 21/25 Training loss: 4.161419691288308 Validation loss: 5.013389796426852 Validation perplexity: 153.5675761779192\n",
            "\n",
            "\n",
            "Time: 3m 36s Epoch: 22/25 Training loss: 4.10425225430972 Validation loss: 5.007696406482017 Validation perplexity: 152.78236045033336\n",
            "\n",
            "\n",
            "Time: 3m 46s Epoch: 23/25 Training loss: 4.046860938202845 Validation loss: 5.033798381073834 Validation perplexity: 156.88207505373794\n",
            "\n",
            "\n",
            "Time: 3m 56s Epoch: 24/25 Training loss: 3.9917650149293142 Validation loss: 5.025209328899645 Validation perplexity: 155.62215134281425\n",
            "\n",
            "\n",
            "Time: 4m 6s Epoch: 25/25 Training loss: 3.9403829394954526 Validation loss: 5.016575454032584 Validation perplexity: 154.17013983908802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORArxLEimswD",
        "colab_type": "text"
      },
      "source": [
        "## Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6v9GnObpoS",
        "colab_type": "text"
      },
      "source": [
        "*Fixed k*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QrQaaQTw-o8",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k, out_size):\n",
        "    \n",
        "    net.eval()\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    \n",
        "    for w in words:\n",
        "        ix = torch.LongTensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "    words.append(int_to_vocab[choice])\n",
        "    for _ in range(out_size):\n",
        "        ix = torch.LongTensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c), _= net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "    print('\\n')    \n",
        "    out = ' '.join(words)\n",
        "    out = out.replace(' .','.')\n",
        "    out = '. '.join(i.capitalize() for i in out.split(\". \"))\n",
        "    print(out) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4S0EultBBQ",
        "colab_type": "code",
        "outputId": "faa2976b-b966-4477-bb0d-cf8861ccdf71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "predict(device,net,['there','was'], n_vocab, vocab_to_int, int_to_vocab, 5, 100)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There was a small, wooden noise. It had gone to a halt with the giant slide out. He could look as though she was going out. It took his hand to her feet on her head with the other two boys. It seemed a little noise of her fingers. Harry and ron were both staring at the other two of the castle at hogwarts. They walked down on his shoulder and the roll of his fellow the school were right behind them. They heard a distant noise and a roll in a faint corner, a large\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjpeaSX4GWK1",
        "colab_type": "text"
      },
      "source": [
        "*Adjusted k*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "agLdl4Low-pQ",
        "colab": {}
      },
      "source": [
        "## Predict method\n",
        "\n",
        "def predict_2(device, net, words, n_vocab, vocab_to_int, int_to_vocab, prob, outsize):\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    \n",
        "    for w in words:\n",
        "        ix = torch.LongTensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
        "\n",
        "        output = F.softmax(output,dim=-1) \n",
        "        sort = output.sort(dim=1, descending=True)[0].cpu().detach().numpy()\n",
        "        \n",
        "  \n",
        "        if np.cumsum(sort)[0]> prob: \n",
        "          top_k = 1\n",
        "        else:\n",
        "          top_k = int(np.max(np.nonzero(np.cumsum(sort)<prob)[0]))\n",
        "\n",
        " \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    #Choose the number of words that we want to predict\n",
        "    \n",
        "    for _ in range(outsize):\n",
        "        ix = torch.LongTensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
        "        output = F.softmax(output,dim=-1) \n",
        "        \n",
        "        sort = output.sort(dim=-1, descending=True)[0].cpu().detach().numpy()\n",
        "\n",
        "        if np.cumsum(sort)[0]> prob or np.cumsum(sort)[1]> prob: \n",
        "          top_k = 1\n",
        "        else:\n",
        "          top_k = int(np.max(np.nonzero(np.cumsum(sort)<prob)[0]))\n",
        "    \n",
        "    \n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        \n",
        "        choice = np.random.choice(choices[0])\n",
        "            \n",
        "        words.append(int_to_vocab[choice])\n",
        "    \n",
        "    print('\\n')     \n",
        "    out = ' '.join(words)\n",
        "    out = out.replace(' .', '.')\n",
        "    out = '. '.join(i.capitalize() for i in out.split(\". \"))\n",
        "    print(out) \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KnqatoloUT3",
        "colab_type": "code",
        "outputId": "5c595b1e-de23-4069-a50b-4ee763fa56e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "predict_2(device,net,['ron','cried'], n_vocab, vocab_to_int, int_to_vocab, 0.35, 200)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Ron cried sharply. \" you don't know what i was going to do. \" \" i'm not looking at me ! \" said hermione. \" don't be in a matter of the time. \" \" yeah, don't know what i know what i was going to do. \" \" don't -- i don't know about the flobberworms?\" said hermione. \" it's a unicorn with a stone -- it was the worst of erised. \" \" yeah, i'm going to be stupid,\" said ron. \" what do you see what he is?\" \" that's not going to be okay. \" \" but you can't see how much -- ?\" \" i'm not a boy ! \" said hermione. \" you can see the stone, you know what the grim are -- ?\" \" i'm going to do you go to the library, harry -- \" said hermione. \" we know what you're going to be on. \" \" yeah,\" said harry. \" that's what he wanted to do. \" \" that's why you're supposed to be a good teacher,\" said ron. \" but it was probably really a baby\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}