{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Harry_Potter_THE BEST MODEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NPRug3HOw-oX"
      },
      "source": [
        "## LSTM Language Model for Harry Potter (three books)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJASEzg7w-oY",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import chainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import re\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Hyperparameters\n",
        "seq_size = 35\n",
        "batch_size = 20\n",
        "embedding_size =650\n",
        "lstm_size = 650\n",
        "num_layers = 2\n",
        "gradients_norm = 5\n",
        "top_k = 5\n",
        "epochs = 50\n",
        "\n",
        "# For optimizer\n",
        "lr = 0.001\n",
        "weight_decay = 2e-5 # L2 regularization\n",
        "\n",
        "\n",
        "#Dropouts\n",
        "dropout=0.5 #locked\n",
        "dropouti=0.5 #locked\n",
        "dropoute=0.1 #emb dropout\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN9ENWQvbXx1",
        "colab_type": "text"
      },
      "source": [
        "## Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avLFVK0nvuTe",
        "colab_type": "code",
        "outputId": "1b25a4c7-9fa7-46ba-9617-1ec2605aa15f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Harry Potter Data\n",
        "\n",
        "from google.colab import files\n",
        "train_file = files.upload()\n",
        "\n",
        "\n",
        "train_file = 'HP.txt'\n",
        "checkpoint_path = 'checkpoint'\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file,'r',encoding=\"utf8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "#        text = text[:int(len(text))]\n",
        "        pat = re.compile(r\"([.()!])\")\n",
        "        text = pat.sub(\" \\\\1 \", text)\n",
        "        text = text.replace('\"','\" ')\n",
        "        text = text.lower().split()\n",
        "      \n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    print('Total size', len(int_text))\n",
        "\n",
        "    #TRAINING SET\n",
        "\n",
        "    int_text0 = int_text[:int(len(int_text)*0.8)]\n",
        "    num_batches = int(len(int_text0) / (seq_size * batch_size))\n",
        "    in_text = int_text0[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    print('Training size', len(int_text0))\n",
        "\n",
        "    #VALIDATION SET\n",
        "\n",
        "    int_text1 = int_text[int(len(int_text)*0.8):]\n",
        "    num_batches = int(len(int_text1) / (seq_size * batch_size))\n",
        "    in_text1 = int_text1[:num_batches * batch_size * seq_size]\n",
        "    out_text1 = np.zeros_like(in_text1)\n",
        "    out_text1[:-1] = in_text1[1:]\n",
        "    out_text1[-1] = in_text1[0]\n",
        "    in_text_val = np.reshape(in_text1, (batch_size, -1))\n",
        "    out_text_val = np.reshape(out_text1, (batch_size, -1))\n",
        "    print('Validation size', len(int_text1))\n",
        "\n",
        "                              \n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val\n",
        "\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, in_text_val, out_text_val= get_data_from_file(train_file, batch_size, seq_size)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dcae4a80-803d-4888-a875-7e3ca604872d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dcae4a80-803d-4888-a875-7e3ca604872d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving HP.txt to HP.txt\n",
            "Vocabulary size 16957\n",
            "Total size 255819\n",
            "Training size 204655\n",
            "Validation size 51164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5RpMeSXbbNG",
        "colab_type": "text"
      },
      "source": [
        "## Define regularization functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0T07ZeWcfgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2k81eF6c0vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
        "  if dropout:\n",
        "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n",
        "    masked_embed_weight = mask * embed.weight\n",
        "  else:\n",
        "    masked_embed_weight = embed.weight\n",
        "\n",
        "  padding_idx = embed.padding_idx\n",
        "  if padding_idx is None:\n",
        "      padding_idx = -1\n",
        "\n",
        "  X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
        "    padding_idx, embed.max_norm, embed.norm_type,\n",
        "    embed.scale_grad_by_freq, embed.sparse\n",
        "  )\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV_0rnWRbhsW",
        "colab_type": "text"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FViF_Gx1bcAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_vocab, seq_size, \n",
        "                 embedding_size, lstm_size, num_layers, dropout=0.5, dropouti=0.5, dropoute=0.1):\n",
        "        super(RNNModule, self).__init__()\n",
        "       \n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lockdrop = LockedDropout()\n",
        "        self.idrop = nn.Dropout(dropouti)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_size, \n",
        "                            num_layers, batch_first=True)\n",
        "      \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.dropouti = dropouti\n",
        "        self.dropoute = dropoute\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.dense.bias.data.fill_(0)\n",
        "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def forward(self, x, prev_state):\n",
        "        embed = embedded_dropout(self.embedding, x, dropout=self.dropoute if self.training else 0)\n",
        "        embed = self.idrop(embed)\n",
        "        embed = self.lockdrop(embed, self.dropouti)\n",
        "\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        output = self.lockdrop(output, self.dropout)\n",
        "\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output, state, embed\n",
        "    \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, \n",
        "                            self.lstm_size),\n",
        "                torch.zeros(self.num_layers, batch_size, \n",
        "                            self.lstm_size))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qwwoIBzcw-ov",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lr, weight_decay=weight_decay):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr, weight_decay=weight_decay)\n",
        "    return criterion, optimizer\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Zpv2EfmlSz",
        "colab_type": "text"
      },
      "source": [
        "## Training and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ5_cCKgx6nH",
        "outputId": "7fefdbe0-7e19-49f6-8b30-bd1f76534e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TRAINING\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Initialize a new network\n",
        "net = RNNModule(n_vocab, seq_size, embedding_size, lstm_size, num_layers,dropout, dropouti, dropoute)\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "criterion, optimizer = get_loss_and_train_op(net, lr,weight_decay=weight_decay)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "training_loss, validation_loss, validation_perplex = [], [], []\n",
        "\n",
        "# For each epoch\n",
        "for e in range(1,epochs+1):\n",
        "  \n",
        "    losses_train, losses_val, perplexity_val = [], [], []\n",
        "\n",
        "    batches_val = get_batches(in_text_val, out_text_val, batch_size, seq_size)\n",
        "    state_h, state_c = net.zero_state(batch_size)\n",
        "\n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "      \n",
        "    # For each sentence in validation set\n",
        "    for x,y in batches_val:\n",
        "                  \n",
        "        # Tell it we are in eval mode\n",
        "        net.eval()\n",
        "\n",
        "        # Make tensors\n",
        "        x = torch.LongTensor(x).to(device) # inputs\n",
        "        y = torch.LongTensor(y).to(device) # targets\n",
        "          \n",
        "        #Forward pass \n",
        "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
        "        loss = criterion(outputs.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "\n",
        "        # Compute loss and perplexity\n",
        "        loss_value = loss.item()\n",
        "        losses_val.append(loss_value)\n",
        "        perplex = math.exp(loss_value)\n",
        "        perplexity_val.append(perplex)\n",
        "      \n",
        "    batches = get_batches(in_text, out_text, batch_size,seq_size)\n",
        "    state_h, state_c = net.zero_state(batch_size)\n",
        "    \n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    # For each sentence in training set\n",
        "    for x,y in batches:\n",
        "        \n",
        "        # Tell it we are in training mode\n",
        "        net.train()\n",
        "        \n",
        "        # Reset all gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Make tensors\n",
        "        x = torch.LongTensor(x).to(device) # inputs\n",
        "        y = torch.LongTensor(y).to(device) # targets\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs, (state_h, state_c),_ = net(x, (state_h, state_c))\n",
        "        loss = criterion(outputs.transpose(1, 2), y)\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "        \n",
        "        loss_value = loss.item()\n",
        "        losses_train.append(loss_value)\n",
        "        \n",
        "        # Perform back-propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
        "\n",
        "        # Update the network's parameters\n",
        "        optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Save loss and perplexity for plot\n",
        "    training_loss.append(np.mean(losses_train))\n",
        "    validation_loss.append(np.mean(losses_val))\n",
        "    validation_perplex.append(np.mean(perplexity_val))\n",
        "     \n",
        "    # Print at every epoch    \n",
        "    if e % 1 == 0:\n",
        "        print('\\n') \n",
        "        print('Time: {}'.format(time_since(start)),\n",
        "              'Epoch: {}/{}'.format(e, epochs),\n",
        "              'Training loss: {}'.format(training_loss[-1]),\n",
        "              'Validation loss: {}'.format(validation_loss[-1]),\n",
        "              'Validation perplexity: {}'.format(validation_perplex[-1]))\n",
        "   "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Time: 0m 9s Epoch: 1/50 Training loss: 6.6569130045093905 Validation loss: 9.731058329752047 Validation perplexity: 16832.36828682614\n",
            "\n",
            "\n",
            "Time: 0m 19s Epoch: 2/50 Training loss: 6.108396339089903 Validation loss: 5.900929026407738 Validation perplexity: 369.60276827130355\n",
            "\n",
            "\n",
            "Time: 0m 29s Epoch: 3/50 Training loss: 5.870442579870355 Validation loss: 5.620118500435189 Validation perplexity: 279.66786052108336\n",
            "\n",
            "\n",
            "Time: 0m 39s Epoch: 4/50 Training loss: 5.699160714672036 Validation loss: 5.481633630517411 Validation perplexity: 243.7182827264894\n",
            "\n",
            "\n",
            "Time: 0m 49s Epoch: 5/50 Training loss: 5.555358321699377 Validation loss: 5.399273852779441 Validation perplexity: 224.649555337502\n",
            "\n",
            "\n",
            "Time: 0m 59s Epoch: 6/50 Training loss: 5.423337083973297 Validation loss: 5.331285221935952 Validation perplexity: 210.04450879665526\n",
            "\n",
            "\n",
            "Time: 1m 8s Epoch: 7/50 Training loss: 5.307050629837872 Validation loss: 5.274910959478927 Validation perplexity: 198.71736280561277\n",
            "\n",
            "\n",
            "Time: 1m 18s Epoch: 8/50 Training loss: 5.204639266615045 Validation loss: 5.229300485898371 Validation perplexity: 190.0072401373837\n",
            "\n",
            "\n",
            "Time: 1m 28s Epoch: 9/50 Training loss: 5.087886230586326 Validation loss: 5.193250806364294 Validation perplexity: 183.28042029822612\n",
            "\n",
            "\n",
            "Time: 1m 38s Epoch: 10/50 Training loss: 4.990740767896992 Validation loss: 5.156408440576841 Validation perplexity: 176.72967327023505\n",
            "\n",
            "\n",
            "Time: 1m 48s Epoch: 11/50 Training loss: 4.889222793383141 Validation loss: 5.135129791416534 Validation perplexity: 173.12943436655533\n",
            "\n",
            "\n",
            "Time: 1m 58s Epoch: 12/50 Training loss: 4.789146542549133 Validation loss: 5.105768151479225 Validation perplexity: 168.2186170369395\n",
            "\n",
            "\n",
            "Time: 2m 7s Epoch: 13/50 Training loss: 4.708888440915983 Validation loss: 5.080239230639314 Validation perplexity: 164.06119775281894\n",
            "\n",
            "\n",
            "Time: 2m 17s Epoch: 14/50 Training loss: 4.629596757562193 Validation loss: 5.065988227112652 Validation perplexity: 161.61351255320838\n",
            "\n",
            "\n",
            "Time: 2m 27s Epoch: 15/50 Training loss: 4.551108926942904 Validation loss: 5.05052375793457 Validation perplexity: 159.17657157854939\n",
            "\n",
            "\n",
            "Time: 2m 37s Epoch: 16/50 Training loss: 4.470819481431621 Validation loss: 5.045948701362088 Validation perplexity: 158.5386092343604\n",
            "\n",
            "\n",
            "Time: 2m 47s Epoch: 17/50 Training loss: 4.408240976398939 Validation loss: 5.033831387350004 Validation perplexity: 156.63894069034055\n",
            "\n",
            "\n",
            "Time: 2m 56s Epoch: 18/50 Training loss: 4.341830662668568 Validation loss: 5.0393542132965505 Validation perplexity: 157.53107206910173\n",
            "\n",
            "\n",
            "Time: 3m 6s Epoch: 19/50 Training loss: 4.261837704540932 Validation loss: 5.039563100631923 Validation perplexity: 157.6942304181865\n",
            "\n",
            "\n",
            "Time: 3m 16s Epoch: 20/50 Training loss: 4.207580763183228 Validation loss: 5.029255070098459 Validation perplexity: 155.98075801453547\n",
            "\n",
            "\n",
            "Time: 3m 26s Epoch: 21/50 Training loss: 4.144283917668748 Validation loss: 5.043010280556874 Validation perplexity: 158.33981896046825\n",
            "\n",
            "\n",
            "Time: 3m 36s Epoch: 22/50 Training loss: 4.091165930440981 Validation loss: 5.028692532892096 Validation perplexity: 156.11489353102004\n",
            "\n",
            "\n",
            "Time: 3m 46s Epoch: 23/50 Training loss: 4.040401729818893 Validation loss: 5.024407262671484 Validation perplexity: 155.45881556352342\n",
            "\n",
            "\n",
            "Time: 3m 55s Epoch: 24/50 Training loss: 3.9788120535955036 Validation loss: 5.024063841937339 Validation perplexity: 155.34806619877904\n",
            "\n",
            "\n",
            "Time: 4m 5s Epoch: 25/50 Training loss: 3.926734210693673 Validation loss: 5.027930109468225 Validation perplexity: 155.98284775023913\n",
            "\n",
            "\n",
            "Time: 4m 15s Epoch: 26/50 Training loss: 3.8852920989467674 Validation loss: 5.031472722144976 Validation perplexity: 156.4719695018482\n",
            "\n",
            "\n",
            "Time: 4m 25s Epoch: 27/50 Training loss: 3.831495111935759 Validation loss: 5.057270709782431 Validation perplexity: 160.6048217042906\n",
            "\n",
            "\n",
            "Time: 4m 35s Epoch: 28/50 Training loss: 3.8004360958321453 Validation loss: 5.0328622974761545 Validation perplexity: 156.71792487431702\n",
            "\n",
            "\n",
            "Time: 4m 44s Epoch: 29/50 Training loss: 3.7493735315048533 Validation loss: 5.033887307937831 Validation perplexity: 157.014246536719\n",
            "\n",
            "\n",
            "Time: 4m 54s Epoch: 30/50 Training loss: 3.724222508195328 Validation loss: 5.032084791627649 Validation perplexity: 156.66669995242898\n",
            "\n",
            "\n",
            "Time: 5m 4s Epoch: 31/50 Training loss: 3.6660516768285674 Validation loss: 5.029497015966128 Validation perplexity: 156.33069302777895\n",
            "\n",
            "\n",
            "Time: 5m 14s Epoch: 32/50 Training loss: 3.6545306762603866 Validation loss: 5.037403694570881 Validation perplexity: 157.57743789444515\n",
            "\n",
            "\n",
            "Time: 5m 24s Epoch: 33/50 Training loss: 3.5942028181193626 Validation loss: 5.064078997259271 Validation perplexity: 161.91915207280925\n",
            "\n",
            "\n",
            "Time: 5m 34s Epoch: 34/50 Training loss: 3.568784342236715 Validation loss: 5.033883147043724 Validation perplexity: 157.10573900451988\n",
            "\n",
            "\n",
            "Time: 5m 43s Epoch: 35/50 Training loss: 3.541103369569125 Validation loss: 5.045662899539895 Validation perplexity: 158.84403515084054\n",
            "\n",
            "\n",
            "Time: 5m 53s Epoch: 36/50 Training loss: 3.475013603086341 Validation loss: 5.051715243352603 Validation perplexity: 159.88439767491428\n",
            "\n",
            "\n",
            "Time: 6m 3s Epoch: 37/50 Training loss: 3.4706613413275105 Validation loss: 5.068431514583222 Validation perplexity: 162.60763846551137\n",
            "\n",
            "\n",
            "Time: 6m 13s Epoch: 38/50 Training loss: 3.442130535432737 Validation loss: 5.07960765002525 Validation perplexity: 164.49730969606077\n",
            "\n",
            "\n",
            "Time: 6m 23s Epoch: 39/50 Training loss: 3.4017777761367904 Validation loss: 5.0776304480147685 Validation perplexity: 164.2032887862845\n",
            "\n",
            "\n",
            "Time: 6m 32s Epoch: 40/50 Training loss: 3.375763798413211 Validation loss: 5.079298522374401 Validation perplexity: 164.4258013393915\n",
            "\n",
            "\n",
            "Time: 6m 42s Epoch: 41/50 Training loss: 3.356477483494641 Validation loss: 5.093042772110194 Validation perplexity: 166.8818724857312\n",
            "\n",
            "\n",
            "Time: 6m 52s Epoch: 42/50 Training loss: 3.336341054472205 Validation loss: 5.096120357513428 Validation perplexity: 167.38447655037447\n",
            "\n",
            "\n",
            "Time: 7m 2s Epoch: 43/50 Training loss: 3.303640795897131 Validation loss: 5.100495985109512 Validation perplexity: 168.07873463940004\n",
            "\n",
            "\n",
            "Time: 7m 12s Epoch: 44/50 Training loss: 3.2871176763756633 Validation loss: 5.123446098745686 Validation perplexity: 171.935970362103\n",
            "\n",
            "\n",
            "Time: 7m 21s Epoch: 45/50 Training loss: 3.244194986885541 Validation loss: 5.104363160590603 Validation perplexity: 168.68974379969754\n",
            "\n",
            "\n",
            "Time: 7m 31s Epoch: 46/50 Training loss: 3.2508327152631056 Validation loss: 5.126564835848874 Validation perplexity: 172.50455666027997\n",
            "\n",
            "\n",
            "Time: 7m 41s Epoch: 47/50 Training loss: 3.21726092737015 Validation loss: 5.134318057804892 Validation perplexity: 173.95398047247878\n",
            "\n",
            "\n",
            "Time: 7m 51s Epoch: 48/50 Training loss: 3.180495209889869 Validation loss: 5.130291265984104 Validation perplexity: 173.31700439173488\n",
            "\n",
            "\n",
            "Time: 8m 1s Epoch: 49/50 Training loss: 3.1608859292448384 Validation loss: 5.126681948361331 Validation perplexity: 172.60266962067377\n",
            "\n",
            "\n",
            "Time: 8m 11s Epoch: 50/50 Training loss: 3.1478678633088935 Validation loss: 5.127831988138695 Validation perplexity: 172.918249148292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORArxLEimswD",
        "colab_type": "text"
      },
      "source": [
        "## Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6v9GnObpoS",
        "colab_type": "text"
      },
      "source": [
        "*Fixed k*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QrQaaQTw-o8",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k, out_size):\n",
        "    \n",
        "    net.eval()\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    \n",
        "    for w in words:\n",
        "        ix = torch.LongTensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "    words.append(int_to_vocab[choice])\n",
        "    for _ in range(out_size):\n",
        "        ix = torch.LongTensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c), _= net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "    print('\\n')    \n",
        "    out = ' '.join(words)\n",
        "    out = out.replace(' .','.')\n",
        "    out = '. '.join(i.capitalize() for i in out.split(\". \"))\n",
        "    print(out) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4S0EultBBQ",
        "colab_type": "code",
        "outputId": "3fdf04cb-960e-47df-e5a7-63cf53ede3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "predict(device,net,['harry','had'], n_vocab, vocab_to_int, int_to_vocab, 5, 200)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Harry had heard ron gasp in front of his eyes. The bloody bus turned over and began at all over the board, and they saw that they could hardly meet it in a low, more voice from inside the castle, but there was the only one in harry's family. Harry was quite keen. He could hear his footsteps when he saw something that had just heard. The door of number of a large black tables were hidden under their noses in their first class as the last class of the school in the castle, they didn't know the very last time he would be. \" right then,\" she said to harry, \" i mean, you know that the dementors were so scared to be able to give him permission to get him to gryffindor tower ! but you can look if i can hear them -- i can make it in a good feast. You have to take your parents on the train in my family ! i have no more reason in the world or witches or wizards of the most witches and witches of wizards in a century on a plate for your first class,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjpeaSX4GWK1",
        "colab_type": "text"
      },
      "source": [
        "*Adjusted k*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "agLdl4Low-pQ",
        "colab": {}
      },
      "source": [
        "## Predict method\n",
        "\n",
        "def predict_2(device, net, words, n_vocab, vocab_to_int, int_to_vocab, prob, outsize):\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    \n",
        "    for w in words:\n",
        "        ix = torch.LongTensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
        "\n",
        "        output = F.softmax(output,dim=-1) \n",
        "        sort = output.sort(dim=1, descending=True)[0].cpu().detach().numpy()\n",
        "        \n",
        "  \n",
        "        if np.cumsum(sort)[0]> prob: \n",
        "          top_k = 1\n",
        "        else:\n",
        "          top_k = int(np.max(np.nonzero(np.cumsum(sort)<prob)[0]))\n",
        "\n",
        " \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    #Choose the number of words that we want to predict\n",
        "    \n",
        "    for _ in range(outsize):\n",
        "        ix = torch.LongTensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c),_ = net(ix, (state_h, state_c))\n",
        "        output = F.softmax(output,dim=-1) \n",
        "        \n",
        "        sort = output.sort(dim=-1, descending=True)[0].cpu().detach().numpy()\n",
        "\n",
        "        if np.cumsum(sort)[0]> prob or np.cumsum(sort)[1]> prob: \n",
        "          top_k = 1\n",
        "        else:\n",
        "          top_k = int(np.max(np.nonzero(np.cumsum(sort)<prob)[0]))\n",
        "    \n",
        "    \n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        \n",
        "        choice = np.random.choice(choices[0])\n",
        "            \n",
        "        words.append(int_to_vocab[choice])\n",
        "    \n",
        "    print('\\n')     \n",
        "    out = ' '.join(words)\n",
        "    out = out.replace(' .', '.')\n",
        "    out = '. '.join(i.capitalize() for i in out.split(\". \"))\n",
        "    print(out) \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KnqatoloUT3",
        "colab_type": "code",
        "outputId": "8e7171d8-f19d-45ee-84f3-79e2b8307826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "predict_2(device,net,['harry','had'], n_vocab, vocab_to_int, int_to_vocab, 0.35, 200)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Harry had committed. He was still shaking with his wand and the dark street. He had never seen the giant flames. He had been riding his eyes on the door of the dark window. He had been ill. \" you know what he is?\" said ron, frowning at the lump of the newspaper. \" you know, how could you look at all?\" \" you know what we can't do. \" \" you know what it is?\" said ron, frowning at the class. \" i mean, i was a very good idea for you. \" \" and i don't know what you're talking about ! \" said hermione. \" what do you think it was me?\" \" the sixteenth of october ! \" said hermione. \" well, that's all the way. \" \" what do you think he's going to do with you?\" said hermione. \" i mean, i thought you could be a bit of the old identity to be a little simple. \" \" what do you think he remembers about the first place of all?\" harry asked, watching the rest of the line. \" well, that's\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}