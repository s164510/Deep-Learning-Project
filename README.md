# Deep-Learning-Project

In this project, three LSMT language models have been trained on each of their data. 
The model trained on the Penn Tree Bank data set has been used for comparison reasons and has shown to reach a validation perplexity of 88.2. The same hyperparameters were used for the other data, however did not show same performance. Importantly, no comparison was though possible. 

The data folder holds the data used for text generation, i.e. storylines for Kim Possible and text from the first three Harry Potter books. 

The data for PTB is downloaded from: 
https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt
https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt
https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt
